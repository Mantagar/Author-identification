% By zmienic jezyk na angielski/polski, dodaj opcje do klasy english lub polish
\documentclass[polish,12pt]{aghthesis}
\usepackage{polski}
\usepackage[latin2]{inputenc}
\usepackage{url}
\usepackage{graphicx}
\graphicspath{ {images/} }

\author{Marcin Radzio}

\titlePL{Weryfikacja autorstwa z wykorzystaniem rekurencyjnych sieci neuronowych}
\titleEN{Author identification with reccurent neural networks}

\fieldofstudy{Informatyka}

\supervisor{dr \ in¿.\ Marcin Kuta, AGH}

\date{2017}

% Szablon przystosowany jest do druku dwustronnego, 
\begin{document}

\maketitle
\tableofcontents
\newpage
\section{Cel prac i wizja produktu}
%\section{Project goals and vision}
\label{sec:cel-wizja}
\subsection{Charakterystyka problemu}
Zadanie weryfikacji autorstwa polega na zdeterminowaniu, czy autor zbioru
znanych tekstów jest tak¿e autorem pewnego kwestionowanego tekstu. Zadanie to zosta³o zdefiniowane w ten sposób w konkursie PAN 2015, a jednym z zaproponowanych rozwi±zañ by³o wykorzystanie wielog³owych, rekurencyjnych sieci neuronowych, autorstwa Douglasa Bagnalla. Rozwi±zanie to okaza³o siê szczególnie efektywne w warunkach konkursu, osi±gaj±c najwy¿sz± dok³adno¶æ weryfikacji. Zaproponowana sieæ rekurencyjna okaza³a siê lepszym rozwi±zaniem od podej¶æ opartych na tradycyjnych algorytmach klasyfikacji i preprocesingu, przedstawionych przez pozosta³e zespo³y.
\subsection{Motywacja projektu}
W ostatnich latach mogli¶my do¶wiadczyæ ogromnego postêpu w dziedzinie uczenia maszynowego, a przede wszystkim w ¶wiecie sieci neuronowych. Powsta³o wiele zaawansowanych bibliotek do uczenia g³êbokiego (Torch, Theano, TensorFlow), które pozwoli³y na ³atwe tworzenie nawet skomplikowanych sieci, wci±¿ zapewniaj±c szybk± naukê i kontrolê nad jej przebiegiem. Wraz z nimi pojawi³y siê biblioteki dedykowane dla pewnych zastosowañ, jak 'torch-rnn', który daje mo¿liwo¶æ uczenia rekurencyjnych sieci neuronowych dla znakowego modelu jêzykowego (ang. \emph{character level language model RNN}). Jednak¿e wci±¿ brakuje bibliotek, które pozwoli³yby na trenowanie sieci dla zadania weryfikacji autorstwa tekstu pisanego.
Przedmiotem tej pracy jest napisanie takiej w³a¶nie biblioteki. W zamierzeniach stworzona biblioteka umo¿liwiæ stworzenie modelu charakteryzuj±cego siê wy¿sz± dok³adno¶ci± weryfikacji ni¿ obecnie stosowane sieci rekurencyjne stosowane dla tego problemu.
\subsection{Wizja produktu}
Najwa¿niejszym za³o¿eniem projektu jest dostarczenie biblioteki, która pozwoli na konfigurowalne tworzenie g³êbokich sieci typu RNN, trening, preprocesing danych wej¶ciowych, zapewni persystencjê wytrenowanych modeli oraz ich dalsze testowanie. Zak³ada siê, ¿e system powinien wspieraæ naukê w co najmniej czterech jêzykach: angielskim, greckim, hiszpañskim i holenderskim. Wy¿ej wspomniana konfigurowalno¶æ powinna pozwalaæ na tworzenie dowolnych sieci parametryzowanych hiperparametrami: g³êboko¶æ sieci, wymiar wektorów wej¶ciowych, ilo¶æ neuronów w warstwach ukrytych, wielko¶æ mini-batchów, itd. Wa¿na jest te¿ mo¿liwo¶æ wizualizacji modelu, np. poprzez generacjê grafu obrazuj±cego strukturê sieci. Preprocesing powinien minimalizowaæ wielko¶æ alfabetu wej¶ciowego bez utraty charakterystyk tekstu.
TEKST O WARIANTACH TODO
\subsection{Analiza ryzyka}
\subsubsection{Czas testowania}
Produkcja takiej biblioteki wi±¿e siê z nieustannym testowaniem jako¶ci generowanych sieci, co na ogó³ jest czasoch³onne. Trening sieci nawet niedu¿ych rozmiarów poch³ania znaczn± ilo¶æ zasobów procesora oraz czasu. W celu minimalizacj zagro¿eñ z tej strony wykorzysta³em moc obliczeniow± klastra Prometheus, co umo¿liwi³o badanie wydajno¶ci modeli oraz ewaluacja sieci bez obci±¿ania w³asnego sprzêtu.
\subsubsection{Nieznajomo¶æ jêzyka}
Jêzyk Lua nie by³ mi wcze¶niej znany w praktyce, ale spêdzenie czasu na nauce jego sk³adni i zasad nie by³ czasem straconym, gdy¿ jêzyk jest bardzo ciekawy i w po³±czeniu ze ¶rodowiskiem Torch jest niebywale efektywny w dziedzinie uczenia maszynowego i obliczeñ naukowych.
\subsubsection{Nowa technologia}
Jednym z wiêkszych wyzwañ projektu by³o nauczenie siê korzystania z wa¿niejszych bibliotek, które ¶rodowisko Torch udostêpnia. Ta technologia równie¿ nie by³a mi wcze¶niej znana, ale widzê mnóstwo potencjalnych powodów, aby u¿ywaæ go w przysz³o¶ci.
\subsubsection{Wymagaj±cy temat}
Do¶æ d³ugi czas spêdzi³em próbuj±c zrozumieæ dzia³anie RNN. Z pomoc± przyszed³ mi Alfredo Canzani (jeden z twórców Torcha) ze swoj± seri± filmów Youtube na temat mechaniki sieci neuronowych, w szczególno¶ci tych rekurencyjnych. Studium kodu ¼ród³owego biblioteki \emph{char-rnn} Anthony'ego Karpathy'ego tak¿e da³a mi sporo do¶wiadczenia oraz pomys³ na realizacjê koñcowego produktu.
\subsection{Studium wykonalno¶ci}
Dziêki wykorzystaniu zasobów superkomputera Prometheus wiêkszo¶æ zagro¿eñ zwi±zanych z brakiem zasobów pamiêciowych czy czasu procesora zostanie rozwi±zane i w wiêkszym stopniu bêdê móg³ skupiæ siê na planowaniu i rozwijaniu projektu. Korzy¶ciami wynikaj±cymi z braku zespo³u jest fakt, ¿e czas nie jest tracony na konsultacje pomiêdzy cz³onkami oraz kod wykazuje wiêksz± spójno¶æ, jednak¿e projekt powstaje wolniej. Podsumowuj±c, projekt ma du¿e szanse powodzenia, choæ jego funkcjonalno¶æ mo¿e byæ ograniczona.
\section{Zakres funkcjonalno¶ci}
%\section{Functional scope}
\label{sec:zakres-funkcjonalnosci}
\subsection{Wymagania funkcjonalne}
\begin{itemize}
	\item udostêpnienie mechanizmu przetwarzania tekstów wej¶ciowych w celu minimalizacji alfabetu jêzyka
	\item zapewnienie eksportu danych do postaci binarnej, daj±cej mo¿liwo¶æ wielokrotnego wczytania i wykorzystania ich bez ponownego preprocesingu
	\item mo¿liwo¶æ zdefiniowania i stworzenia dowolnego modelu sieci rekurencyjnej okre¶lonego przez nastêpuj±ce parametry:
	\begin{itemize}
		\item maksymalna ilo¶æ ró¿nych autorów tekstów w modelu
		\item wielko¶æ warstwy ukrytej
		\item poziom g³êboko¶ci sieci
	\end{itemize}
	\item parametryzowanie trenowania sieci poprzez:
	\begin{itemize}
		\item rozmiar mini-batchów danych
		\item wspó³czynnik uczenia
		\item tempo zmniejszania siê wspó³czynnika uczenia (redukcja nastêpuje po przetworzeniu pojedynczej epoki)
		\item maksymaln± ilo¶æ epok
		\item d³ugo¶æ sekwencji znaków, na której uczony bêdzie model
	\end{itemize}
	\item zapewnienie czytelnych logów, w trakcie dzia³ania programu, przedstawiaj±cych postêp uczenia oraz aktualny b³±d uczenia.
\end{itemize}
\subsection{Wymagania niefunkcjonalne}
\begin{itemize}
	\item realizacja projektu w ramach pracy in¿ynierskiej
	\item bazowanie na artyku³ach Douglasa Bagnalla na temat wielog³owych RNN i wykorzystanie napisanego przez niego modu³u preprocesingu tekstu
	\item wykorzystanie jêzyka Lua oraz ¶rodowiskaF Torch
	\item trening, testowanie i walidacja sieci na zbiorach PAN 2014 dla zadania weryfikacji autorstwa
	\item u¿ycie systemu kontroli wersji Git
\end{itemize}
\section{Wybrane aspekty realizacji}
%\section{Selected realization aspects}
\label{sec:wybrane-aspekty-realizacji}
\subsection{Wielog³owicowa g³êboka rekurencyjna sieæ neuronowa (ang. \emph{Multi-headed deep RNN})}
Wielog³owicowa g³êboka rekurencyjna sieæ neuronowa ró¿ni siê od tradycyjnej sieci typu feedforward tym, ¿e zachowuje kontekst, tzn. poprzednie wej¶cie sieci ma wp³yw na jej aktualne wyj¶cie. Pozwala to na uczenie jej na danych sekwencyjnych, takich jak tekst.
\newline
Aby nauczyæ tak± sieæ, nale¿y j± rozwin±æ (ang. \emph{unrolling}), tj. skopiowaæ jej model i pospinaæ ze sob± tak jak na rysunku \ref{fig:shallow}, z zachowaniem zasady, ¿e ka¿da kopia wspó³dzieli swoje parametry z ka¿d± inn±.
\newline
\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{rnn}
	\caption{P³ytka rekurencyjna sieæ neuronowa}
	\label{fig:shallow}
\end{figure}
\newline
Wersja g³êboka sieci rekurencyjnej polega na zwiêkszeniu ilo¶ci stanów ukrytych. Przyk³adowo na rysunku \ref{fig:deep}. przedstawi³em sieæ o dwóch stanach ukrytych x i y. Jest to model wielog³owicowej g³êbokiej sieci rekurencyjnej o g³êboko¶ci 2.
\newline
\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{deep}
	\caption{Wielog³owa g³êboka rekurencyjna sieæ neuronowa}
	\label{fig:deep}
\end{figure}
\newline
Na sam koniec warto wspomnieæ, co czyni sieæ wielog³owicow±. Otó¿ sieæ mo¿e mieæ wiele wyj¶æ wspó³dziel±cych podstawowy model RNN. W tym przypadku ka¿de wyj¶cie (ka¿da g³owica) reprezentuje prawdopodobieñstwa wyst±pienia kolejnego znaku dla danego autora w reakcji na aktualny znak na wej¶ciu.
\newline
Za ca³o¶æ obs³ugi tych sieci odpowiada modu³ \emph{RNN} stworzonej biblioteki. Dostarcza on mo¿liwo¶ci: tworzenia parametryzowanego modelu, rozwijania sieci, uczenia rozwiniêtego modelu, zapisu oraz odczytu ju¿ wytrenowanej sieci.
\newline
W odró¿nieniu od sieci Bagnalla nie u¿ywa³em funkcji aktywacji ReSQRT, lecz tanh. Algorytm uczenia w moim przypadku to zwyk³y SGD (ang. \emph{Stochastic Gradient Descent}), a nie jego optymalizacja Adagrad.
\subsection{Paczki danych}
Aby przy¶pieszyæ operacjê uczenia sieci, nale¿y zastosowaæ paczki danych (ang. \emph{mini-batch}) danych. Je¿eli wprowadzamy na wej¶cie sieci wektor, to otrzymujemy na wyj¶ciu wektor, lecz równie dobrze mo¿na wprowadziæ na wej¶cie macierz z³o¿on± z kilku wektorów i w efekcie otrzymaæ macierz na wyj¶ciu. Takie podej¶cie jest o wiele bardziej wydajne ni¿ proste iterowanie po danych.
\newline
Za przygotowywanie mini-batchów danych odpowiedzialny jest modu³ \emph{BatchManager} stworzonej biblioteki.
\subsection{Preprocesing}
Alfabet tekstu wejciowego jest zbyt du¿y, co w sieciach rekurencyjnych mo¿e skutkowaæ s³ab± generalizacj±. W tym celu stosujê ró¿ne redukcje:
\begin{itemize}
	\item ka¿d± wielk± literê zastêpujê specjalnym prefiksem oraz odpowiadaj±c± mu ma³± liter±
	\item wszystkie nawiasy sprowadzam do jednej formy
	\item znaki, które w danym jêzyku wystêpuj± ze znikom± czêstotliwo¶ci± s± usuwane
	\item oraz inne zale¿ne od jêzyka (angielski, grecki, holenderski, hiszpañski), które rozwi±zuj± specyficzne problemy zwi±zane z alfabetem, jak interpretacja liter arabskich w tekstach greckich.
\end{itemize}
Dodatkowo wielokrotne wyst±pienia danego znaku pod rz±d skracane jest do piêciu wyst±pieñ.
\subsection{Algorytm weryfikacji autorstwa}
Rekurencyjna sieæ neuronowa w teorii powinna lepiej przewidywaæ przebieg tekstu autora tekstów w zbiorze treningowym ni¿ innych. Wielog³owa sieæ posiada wiele g³owic, ka¿da z nich odpowiada za jednego autora. Dla tekstu wej¶ciowego ka¿da z nich produkuje wspó³czynnik weryfikacji, który jest liczb± z przedzia³u [0,1]. Je¿eli wynik weryfikacji dla sprawdzanego autora jest wiêkszy ni¿ ¶rednia arytmetyczna wszystkich wyników, to tekst jest uznawany za napisany przez owego autora. W przeciwnym wypadku tekst sprawdzany autor nie jest uznawany za twórcê nieznanego tekstu.
\section{Organizacja pracy}
\subsection{Metodyka}
Praca nad projektem przebiega³a w modelu iteracyjnym. Mo¿na wyró¿niæ najwa¿niejsze czê¶ci ka¿dej iteracji:
\begin{itemize}
	\item wybranie funkcjonalno¶ci
	\item implementacja
	\item testowanie
\end{itemize}
Po zebraniu wymagañ, ale przed przej¶ciem do planowania, mogê wyró¿niæ dodatkow± fazê - zbierania informacji, gdy¿ musia³em jeszcze nauczyæ siê jêzyka Lua, obs³ugi biblioteki Torch oraz teorii dzia³ania rekurencyjnych sieci neuronowych.
\subsection{Postêpy}
W wyniku zastosowanego modelu oraz hierarchii funkcjonalno¶ci podj±³em siê nastêpuj±cych iteracji:
\begin{enumerate}
	\item definiowanie modelu - napisanie metody umo¿liwiaj±cej utworzenie konfigurowanego parametrami modelu g³êbokiej sieci RNN
	\item wizualizacja oraz rozwiniêcie modelu - dodanie opcji eksportu modelu w postaci grafu oraz zaimplementowanie metody rozwijaj±cej model w czasie
	\item podstawowy preprocesing - przygotowanie systemu przygotowuj±cego tekst, poprzez redukcjê rozmiaru alfabetu i zapis w postaci binarnej (na razie tylko dla jêzyka angielskiego)
	\item trening - zaimplementowanie metody uczenia rozwiniêtej sieci
	\item zapis oraz testy na Prometheusie - dodanie opcji eksportu i importu sieci oraz rozpoczêcie testów na klastrze
	\item wielog³owa sieæ - rozwiniêcie sieci do postaci wielog³owej, poprawa implementacji, wprowadzenie mo¿liwo¶ci uczenia na tekstach wielu autorów
	\item testy na Prometheusie - testowanie wielog³owych sieci
	\item wprowadzenie mini-batchów - implementacja batchera, która przyspieszy³a naukê niemal trzykrotnie dla mini-batchów wielko¶ci 40
	\item pozosta³e jêzyki - dodanie wsparcia dla jêzyków: hiszpañskiego, greckiego i holenderskiego
	\item opcje i kolorowanie - parsowanie argumentów lini poleceñ w celu uzyskania parametrów sieci oraz kolorowanie tekstu w konsoli
	\item zaimplementowanie programu waliduj±cego nauczone sieci
	\item refaktoryzacja, modularyzacja oraz testowanie kodu
\end{enumerate}
%\section{Work organization}
\label{sec:organizacja-pracy}

\section{Wyniki projektu}
%\section{Project results}

\label{sec:wyniki-projektu}

\subsection{Osi±gniêcia}
W wyniku przeprowadzonych prac uda³o siê zaimplementowaæ bibliotekê pozwalaj±c± na tworzenie wielog³owych, g³êbokich sieci neuronowych - ich struktura jest kontrolowana parametrami, trenowanie sieci na sprecjalnie przygotowanych plikach ucz±cych, zapisywanie wytrenowanych sieci oraz testowanie ich wydajno¶ci i skuteczno¶ci.
\subsection{Problemy}
\subsubsection{Eksploduj±cy gradient}
W trakcie treningu sieci wielokrotnie do¶wiadczy³em zjawiska eksploduj±cego gradientu (ang. \emph{vanishing gradient}), który wywo³any by³ nieodpowiednim dobraniem wspó³czynnika uczenia (ang. \emph{learning rate}). Najlepszym sposobem na ominiêcie tego poroblemu okaza³y siê wielokrotne próby z ró¿nymi parametrami, aby dobraæ optymalny, domy¶lny wspó³czynnik, który pozwoli³by na efektywne uczenie modeli.
\subsubsection{Optymalizacja Adagrad}
Wielokrotnie próbowa³em wprowadziæ optymalizacjê algorytmu SGD (ang. \emph{stochastic gradient descent}), niestety wykorzystanie pakietu \emph{optim} powodowa³o, ¿e modele przesta³y siê uczyæ, co mog³o byæ wynikiem specyficzno¶ci wielog³owych rekurencyjnych sieci neuronowych lub wspó³dzielenia wag przez RNN.
\subsection{Wyniki eksperymentów}
Nawet sieci uczone krótko (nie wiêcej ni¿ 5 epok) wykazywa³y skuteczno¶æ weryfikacji autorstwa relatywnie wysok±. Ewaluacja dok³adno¶ci weryfikacji nieznanych tekstów w jêzyku angielskim przedstawiona jest w tabeli \ref{tab:results}.
\begin{table}[h]
	\caption{Wyniki eksperymentów dla tekstów w jêzyku angielskim}
	\label{tab:results}
	\begin{tabular}{|c|c|c|c|c|c|c|c|}
		\hline
		Hidden size & Stacks & Heads & Mini-batch size & Epochs & Learning rate & BPTT steps & Verification score \\
		\hline
		199 & 3 & 10 & 40 & 5 & 0.002 & 20 & \textbf{0.9} \\
		199 & 3 & 20 & 40 & 5 & 0.010 & 20 & \textbf{0.8} \\
		99 & 2 & 10 & 80 & 5 & 0.005 & 50 & \textbf{0.8} \\
		\hline
	\end{tabular}
	\centering
\end{table}
\subsection{Propozycje dalszego rozwoju}
Sieci definiowane przez program u¿ywaj± zwyk³ego algorytmu mini-batch SGD, aby poprawiæ ich wydajno¶æ mo¿na by dodaæ optymalizacje, np. za pomoc± pakietu \emph{optim}.
Dziêki zastosowaniu mini-batchów nauka przebiega o wiele szybciej, choæ z wykorzystaniem karty graficznej i pakietów \emph{cuda} lub \emph{cunn} mo¿liwe jest osi±gniêcie przyspieszenia wielokrotnie wiêkszego.

\section{Dodatki}
\subsection{Spotkania z opiekunem}
\emph{14.03.2017}
\newline
Spotkanie w celu omówienia tematu pracy. Zosta³em zapoznany ze szczegó³ami oraz zosta³y mi przedstawione dokumenty bêd±ce podstaw± tej pracy, jak równie¿ zosta³em poinformowany potrzebie nauczenia siê jêzyka Lua oraz obs³ugi biblioteki Torch.
\newline
\newline
\emph{28.03.2017}
\newline
Spotkanie w celu doprecyzowania szczegó³ów i odpowiedzenia na pytania odno¶nie charakterystyki tematu. Zosta³y mi przedstawione dodatkowe modu³y Torcha pozwalaj±ce na rozwiniêcie projektu mniejszym nak³adem pracy.
\newline
\newline
\emph{23.05.2017}
\newline
Spotkanie prezentuj±ce postêpy w pracach nad projektem. Zosta³em poinformowany o mo¿liwo¶ci wykorzystania klastra Prometheus do testów sieci oraz zosta³em dodany do grantu obliczeniowego zapewniaj±cego dostêp do zasobów superkomputera. Uzgodnili¶my równie¿ wtedy utworzenie repozytorium w systemie GitHub w celu nadzorowania dalszych postêpów prac. Projekt jest dostêpny pod adresem \url{https://github.com/Mantagar/Author-identification}.
\subsection{Spotkania z prowadz±cym pracowniê projektow±}
W VI semestrze odby³y siê cztery spotkania pracowni projektowej.
\begin{enumerate}
	\item Na pierwszym przedstawione zosta³y nam zasady zaliczenia przedmiotu oraz zostali¶my poinformowani o wymaganiach odno¶nie kolejnych zajêæ.
	\item Na drugim spotkaniu prezentowali¶my wizjê naszych prac.
	\item Na trzecim spotkaniu prezentowali¶my analizê ryzyka oraz studium wykonalno¶ci naszych projektów oraz wymagania funkcjonalne i niefunkcjonalne.
	\item Na czwartym i ostanim spotkaniu prezentowali¶my postêpy prac nad projektami oraz prototyp funkcjonalny systemu.
\end{enumerate}
% o ile to mozliwe prosze uzywac odwolan \cite w konkretnych miejscach a nie \nocite

\nocite{deep_learning,db_1,db_2,alfredo,lua,torch,torch_ref,grid,deep,pan}

\bibliography{bibliografia}

\end{document}
